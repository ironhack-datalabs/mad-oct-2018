{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Challenge 1 - Import and Describe the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this challenge we will use the `austin_weather.csv` file. \n",
    "\n",
    "#### First import it into a data frame called `austin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, describe the dataset you have loaded: \n",
    "- Look at the variables and their types\n",
    "- Examine the descriptive statistics of the numeric variables \n",
    "- Look at the first five rows of all variables to evaluate the categorical variables as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given the information you have learned from examining the dataset, write down three insights about the data in a markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Insights:\n",
    "\n",
    "1. There are 21 variables in the dataset. 3 of them are numeric and the rest contain some text.\n",
    "\n",
    "2. The average temperature in Austin ranged between around 70 degrees F and around 93 degrees F. The highest temperature observed during this period was 107 degrees F and the lowest was 19 degrees F.\n",
    "\n",
    "3. When we look at the head function, we see that a lot of variables contain numeric data even though these columns are of object type. This means we might have to do some data cleansing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's examine the DewPointAvgF variable by using the unique() function to identify what makes this column of object type\n",
    "\n",
    "Describe what you find in a markdown cell below the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a list of columns misrepresented as object columns, use this list to \n",
    "# force the columns to numeric using the as_numeric function in the \n",
    "# next cell (make sure to pass the `errors=coerce` argument to the function):\n",
    "\n",
    "# Hint: you may use a loop to change one column at a time but it is more efficient to use the apply function\n",
    "\n",
    "wrong_type_columns = ['DewPointHighF', 'DewPointAvgF', 'DewPointLowF', 'HumidityHighPercent', \n",
    "                      'HumidityAvgPercent', 'HumidityLowPercent', 'SeaLevelPressureHighInches', \n",
    "                      'SeaLevelPressureAvgInches' ,'SeaLevelPressureLowInches', 'VisibilityHighMiles',\n",
    "                      'VisibilityAvgMiles', 'VisibilityLowMiles', 'WindHighMPH', 'WindAvgMPH', \n",
    "                      'WindGustMPH', 'PrecipitationSumInches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that your code worked by running this cell\n",
    "\n",
    "austin.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Handle the Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have fixed the type mismatch, let's address the missing data.\n",
    "\n",
    "By coercing the columns to numeric, we have created NaNs in the cells containing characters. We should choose a strategy to address this missing data.\n",
    "\n",
    "The first step is to examine how many rows contain missing data.\n",
    "\n",
    "We check how much missing data we have by applying the `.isnull()` function to our dataset. To find the rows with missing data, we apply `.any()` to the function and pass the `axis=1` argument. `austin.isnull().any(axis=1)` will return a column containing true if the row contains at least one missing value and false otherwise. Therefore we must subset our dataframe with this column. This will give us all rows with at least one missing value. \n",
    "\n",
    "#### In the next cell, print all rows containing at least one missing value. Use subsetting to obtain the correct number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are multiple strategies to handle missing data. \n",
    "\n",
    "The simplest strategy is to remove all rows containing missing data or all columns containing missing data. This strategy may work in some cases but not others.\n",
    "\n",
    "We can also fill all missing values with a value to indicate that they were missing like 0, -1, or 99. This may work in cases of categorical data. For continuous data some may opt to fill all missing data with the mean. This strategy is not very optimal since it can increase the fit of the model.\n",
    "\n",
    "The last strategy is to fill the values using some algorithm. In our case, we will use linear interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, count the number of rows in the subsetted dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To evaluate how many rows are missing, find the number of rows in the dataset and find the ratio of missing rows to total rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since there is a large proportion of missing data, perhaps we should evaluate which columns have the most missing data and remove those columns. For the remaining columns, we will perform a linear approximation of the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the number of missing rows in each column using the `.isna()` function. We then chain the `.sum` function to the `.isna()` function and find the number of missing rows per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The majority of missing data is in one column. We prefer to remove this column then to fill the missing values in this column since there are too many missing values. \n",
    "\n",
    "Remove this column from the dataframe using the `.drop()` function. Use the `inplace=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will perform linear interpolation of the missing data.\n",
    "\n",
    "This means that we will use a linear algorithm to estimate the missing data. Linear interpolation assumes that there is a straight line between the points and the missing point will fall on that line. This is a good enough approximation for weather related data. Weather related data is typically a time series. Therefore, we do not want to drop rows from our data if possible. It is prefereable to estimate the missing values rather than remove the rows. However, if you have data from a single point in time, perhaps a better solution would be to remove the rows. \n",
    "\n",
    "If you would like to read more about linear interpolation, you can do so [here](https://en.wikipedia.org/wiki/Linear_interpolation).\n",
    "\n",
    "In the following cell, use the `.interpolate()` function on the entire dataframe. Pass the `inplace=True` argument to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that your dataframe contains no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Processing the Text Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our dataframe contains one true text column - the Events column. We should evaluate this column to determine how to process it.\n",
    "\n",
    "Use the `value_counts()` function to evaluate the contents of this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the largest number of events in one day (events are separated by a comma)\n",
    "\n",
    "Enter your answer in the next markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We would like to evaluate all events separately, therefore we will create dummy variables for all events.\n",
    "\n",
    "We can iterate through each event and create a new column and then assign a 1 to that row if the Events column contains the event and zero otherwise. \n",
    "\n",
    "Note that we do not have to exclude one of the columns since some columns will have zero events and others will have more than one. This means that we do not have one column that is a guaranteed linear combination of the remaining columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a True/False column to indicate whether the Events column contains the word Rain. We do this using the `str.contains()` function. We apply this function to the Events column.\n",
    "\n",
    "Create this column to check if rain is in the Events column in the cell below. The output should be a column containing only True and False values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `np.where()` function, we can set our new column to 0 if the condition above is false and 1 if the condition is true. Do this for all weather events and create a column for each one. The list of weather events is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = ['Snow', 'Fog', 'Rain', 'Thunderstorm']\n",
    "\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the original events column in the cell below. Use the `inplace=True` argument in the cell below. This will remove the Events column from the austin dataframe and leave us with the columns for all 4 events instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Sampling and Holdout Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have processed the data for machine learning, we will sample the data as well as separate the data to test and training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's sample our data. We do this using the `sample()` function. In the cell below sample 1/2 of the rows in our dataset. Assign this sample to a variable called `austin_sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typically when performing a machine learning task, we separate the data into a training set and a test set. \n",
    "\n",
    "We first train the model using only the training set. We check our metrics on the training set. We then apply the model to the test set and check our metrics on the test set as well. If the metrics are significantly more optimal on the training set, then we know we have overfit our model. We will need to revise our model to ensure it will be more applicable to data outside the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cells we will separate the data into a training set and a test set using the `train_test_split()` function in scikit-learn.\n",
    "\n",
    "In scikit-learn, we first separate the data to predictor and response variables. This is the standard way of passing datasets into a model in scikit-learn. In the next cell, assign the `TempAvgF` column to `y` and the remaining columns to `X`.\n",
    "\n",
    "You can do this by first creating a list of column names that do not contain `TempAvgF` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, import `train_test_split` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data to predictor and response variables and imported the `train_test_split()` function, split `X` and `y` into `X_train`, `X_test`, `y_train`, and `y_test`. 80% of the data should be in the training set and 20% in the test set. Enter your code in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While this is common practice for most data, when it comes to time series data, we do not want to randomly select rows from our dataset.\n",
    "\n",
    "This is because many time series algorithms rely on observations having equal time distance between them. In such cases, we typically select the majority of rows as the test data and the last few rows as the training data.\n",
    "\n",
    "In the following cell, compute the number of rows that is 80% of our data and round it to the next integer. Assign this number to `ts_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the first `ts_rows` rows of `X` to `X_ts_train` and the remaining rows to `X_ts_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the first `ts_rows` rows of `y` to `y_ts_train` and the remaining rows to `y_ts_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
