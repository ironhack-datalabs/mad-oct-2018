{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will find in this notebook some scrapy exercises to practise your scraping skills**.<br>**Remember:**\n",
    "- **To get each request status code to ensure you get the proper response from the web***\n",
    "- **To print the response text in each request to evaluate the what kind of info you are getting and its format.** \n",
    "- **To check for patterns in the response text to extract the data/info requested in each question.**\n",
    "- **To visit each url and take a look on its code through Chrome developer tool.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the libraries and modules you will need are included below. Feel free to explore other libraries i.e. scrapy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import scrapy\n",
    "from lxml import html\n",
    "from lxml.html import fromstring\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def decoden(l_mensaje): \n",
    "    res = []\n",
    "    for s in l_mensaje: \n",
    "        res.append( base64.b64decode(s).decode(\"utf-8\").replace(\"\\n\",\"\") )\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Download and display the content of robot.txt for Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [here](http://www.robotstxt.org/robotstxt.html) to know more about ***robot.txt***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = \"https://en.wikipedia.org/robots.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_robot = requests.get(url).content\n",
    "soup_robot = BeautifulSoup(html_robot, \"lxml\")\n",
    "soup_robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the name of the most recently added dataset on data.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='http://catalog.data.gov/dataset?q=&sort=metadata_created+desc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "html_gov = requests.get(url).content\n",
    "soup_gov = BeautifulSoup(html_gov, \"lxml\")\n",
    "soup_gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.w3schools.com/cssref/css_selectors.asp\n",
    "\n",
    "# a = [ e.text.strip() for e in soup.select('.sortable.wikitable tr td')]\n",
    "# Como es sólo el primero y están ordenados de más recientes a menos, seleccionamos el [0]\n",
    "[e.text.strip() for e in soup_gov.select('.dataset-heading')][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of datasets currently listed on data.gov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://www.data.gov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_g = requests.get(url).content\n",
    "soup_g = BeautifulSoup(html_g, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_g.select('a[href^=\"/metrics\"]')[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "dominio = 'https://en.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_wd = requests.get(url).content\n",
    "soup_wd = BeautifulSoup(html_wd, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_img_paths = [dominio + e.attrs['href'] for e in soup_wd.select('.image')]\n",
    "lista_img_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "dominio = 'https://en.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_py = requests.get(url).content\n",
    "soup_py = BeautifulSoup(html_py, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_links = [dominio + e.attrs['href'] for e in soup_py.select('a') if 'href' in e.attrs]\n",
    "lista_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "dominio = 'http://uscode.house.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_ch = requests.get(url).content\n",
    "soup_ch = BeautifulSoup(html_ch, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_changes = [e.text.strip() for e in soup_ch.select('.usctitlechanged') ]\n",
    "lista_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "html_most = requests.get(url).content\n",
    "soup_most = BeautifulSoup(html_most, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mejorable para no usar \n",
    "lista_most = [e for e in soup_most.select('.title a') ]\n",
    "most_wanted = [p.text for p in lista_most[1:]]\n",
    "most_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_earth = requests.get(url).content\n",
    "soup_earth = BeautifulSoup(html_earth, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_earth = [ e.text for e in soup_earth.select('#tbody tr td' )]\n",
    "lista_earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que mejorar poner NSEO en latitud y longitud\n",
    "\n",
    "lista_region_lat = lista_earth[4::13]\n",
    "lista_region_lat = [lat[0: -1] for lat in lista_region_lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_region_lon = lista_earth[6::13]\n",
    "lista_region_lon = [lon[0: -1] for lon in lista_region_lon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_region_datetime = lista_earth[3::13]\n",
    "lista_region_datetime = lista_earth[12::13]\n",
    "# lista_region_date = [e[10:20] for e in lista_region_datetime]\n",
    "lista_region_date = [e[0: 10] for e in lista_region_datetime]\n",
    "\n",
    "lista_region_time = [e[11:] for e in lista_region_datetime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_region_region = lista_earth[11::13]\n",
    "lista_region_region = [reg[1:] for reg in lista_region_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = lista_region_date\n",
    "time = lista_region_time\n",
    "latitude = lista_region_lat\n",
    "longitude = lista_region_lon\n",
    "region = lista_region_region\n",
    "\n",
    "\n",
    "eq = list(zip(date, time, latitude, longitude, region))\n",
    "df = pd.DataFrame(eq, columns = ['date', 'time', 'latitude', 'longitude', 'region'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Display the date, days, title, city, country of next 25 Hackevents as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html_hack = requests.get(url).content\n",
    "soup_hack = BeautifulSoup(html_hack, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_cities = soup_hack.find_all(\"span\", class_=\"city\")\n",
    "# [<p class=\"body strikeout\"></p>]\n",
    "cities = [e.text for e in soup_cities]\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_country = soup_hack.find_all(\"span\", class_=\"country\")\n",
    "# [<p class=\"body strikeout\"></p>]\n",
    "countries = [e.text for e in soup_country]\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_title = soup_hack.find_all(\"a\", class_=\"title\")\n",
    "# [<p class=\"body strikeout\"></p>]\n",
    "titles = [e.text for e in soup_title]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_dates = soup_hack.find_all(\"span\", class_=\"info-date\")\n",
    "# [<p class=\"body strikeout\"></p>]\n",
    "dates = [' '.join(e.text.split()) for e in soup_dates]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_days = soup_hack.find_all(\"div\", class_=\"date-week-days\")\n",
    "# [<p class=\"body strikeout\"></p>]\n",
    "days = [e.text.split() for e in soup_days]\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusionamos\n",
    "hk = list(zip(dates, days, titles, cities, countries))\n",
    "df = pd.DataFrame(hk, columns = ['Date', 'Day', 'Title', 'City', 'Country'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "while True: \n",
    "    account = input('Nombre de la cuenta: ') # 'JuanGomezJurado'    \n",
    "    try: \n",
    "        html_tw = requests.get(url+account).content  \n",
    "        soup_tw = BeautifulSoup(html_tw, \"lxml\")        \n",
    "        l_tweets = [ e for e in soup_tw.find_all(\"span\", class_=\"ProfileNav-value\")]        \n",
    "    except: \n",
    "        print('Ese usuario no es válido')\n",
    "    else:   \n",
    "        res = l_tweets[0].attrs['data-count']\n",
    "        print(res, 'tweets publicados')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-8-0c88d4a7d760>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-0c88d4a7d760>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    l_tweets = [ e for e in soup_tw.find_all(\"span\", data-nav_=\"followers\")]\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "while True: \n",
    "    account = 'JuanGomezJurado' # input('Nombre de la cuenta: ') # 'JuanGomezJurado'  \n",
    "    \n",
    "    # data-nav=\"followers\n",
    "    try: \n",
    "        html_tw = requests.get(url+account).content  \n",
    "        soup_tw = BeautifulSoup(html_tw, \"lxml\")        \n",
    "        l_tweets = [ e for e in soup_tw.find_all(\"span\", class_=\"ProfileNav-value\")]        \n",
    "    except: \n",
    "        print('Ese usuario no es válido')\n",
    "    else:   \n",
    "        res = l_tweets[0].attrs['data-count']\n",
    "        print(res, 'followers')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "\n",
    "html_wiki_lan = requests.get(url).content\n",
    "soup_wiki_lan = BeautifulSoup(html_wiki_lan, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydivs = soup.findAll(\"div\", {\"class\": \"stylelistrow\"})\n",
    "lista_articulos = [e.text.strip().replace('\\xa0', '') for e in soup_wiki_lan.findAll('a', {'class': 'link-box'}) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language names</th>\n",
       "      <th>number of articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Español</td>\n",
       "      <td>1481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1303000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language names number of articles\n",
       "0        English            5734000\n",
       "1        Español            1481000\n",
       "2            日本語            1124000\n",
       "3        Deutsch            2228000\n",
       "4        Русский            1502000\n",
       "5       Français            2047000\n",
       "6       Italiano            1467000\n",
       "7             中文            1026000\n",
       "8      Português            1007000\n",
       "9         Polski            1303000"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for x in lista_articulos: \n",
    "    # print(base64.b64decode(x).decode(\"utf-8\").replace(\"\\n\",\"\"))\n",
    "    res.append(x.replace('+',' ').replace('\\xa0',' ').split())\n",
    "df_wiki = pd.DataFrame(res, columns = ['language names', 'number of articles', 'type'])\n",
    "df_wiki = df_wiki[['language names', 'number of articles']]\n",
    "df_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "html_wiki_datasets = requests.get(url).content\n",
    "soup_wiki_datasets = BeautifulSoup(html_wiki_datasets, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_datasets = [e for e in soup_wiki_datasets.findAll('a', {'class': 'grid-row dgu-topics'}) ]\n",
    "lista_datasets = [e.attrs['href'] for e in soup_wiki_datasets.select('a') if 'href' in e.attrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_datasets = [e for e in lista_datasets if 'search?filters%5Btopic%5D=' in e]\n",
    "lista_datasets = [e.replace('/search?filters%5Btopic%5D=', '').replace('+', ' ') for e in lista_datasets]\n",
    "lista_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. The total number of publications produced by the GAO (U.S. Government Accountability Office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://www.gao.gov/browse/date/custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
